{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Anything you can do AI GAN do better: Training a GAN to generate artwork based on genre</center>\n",
    "### <center> Keanu Spies | CS229A Spring 2018 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Abstract </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>*Recently work has be done to apply Generative Adversarial Networks (GANs) to create AI generated images [1]. This combined with the recent work from Tan *.et al* [4]. that uses textual descriptions of genre to create inputs, allows for better recognizable artwork from GANs. In this work I hypothesize a fully connected automatic generation of artwork by two phases of generation: textual and imagery. With this we hope to create an architecture that is entirely automated and provides more believable artworks than that of standalone GANs.*</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> 1. Introduction </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we develop machines capable of conquering all tasks humans might, an interesting field lies still mostly unconquered - creativity. One such sub-field is the automatic generation of artwork. This project aims to further the field of automatic art generation to produce more believable and aesthetically pleasing artworks.\n",
    "\n",
    "By using a Generative Adversarial Networks (GANs) I will train a Discriminator (D) and a Generator (G) set of neural networks which I will train concurrently to produce unseen and novel works of art. While work has been done on creating artwork using GANs the results are not yet perfect. In this project I will be attempting to develop higher quality visual results with better conceptual meaning and more human-believable results.\n",
    "\n",
    "I will be combining this project with a project in CS231N. In that project I will be exploring the use of conditonal textual input to the GAN in order to create more realistic and believable outputs.\n",
    "\n",
    "#### 1.1 General Overview of the System\n",
    "\n",
    "The Generator of the network (G) is trained such that, given a vector of Gaussian noise, it will generate a work of novel art which is not found in the training set. The Discriminator is trained such that it should be able to recognise the distinction between art/not art. \n",
    "\n",
    "This means that the two networks are working to train the following mini-max game:\n",
    "\n",
    "$$\\min_G \\max_D V(D, G) = \\mathop{\\mathbb{E}}_{x \\tilde p_{data}(x)} [\\text{log}D(x)] + \\mathop{\\mathbb{E}}_{z \\tilde p_{z}(z)} [\\text{log}(1 - D(G(x)))]  $$\n",
    "\n",
    "With this the Generator is attempting to create works that fool the discriminator that they are real, while the discriminator is attempting to get better at classifying fake images. \n",
    "\n",
    "#### 1.2 GAN Architecture\n",
    "\n",
    "The General architecture of a GAN is as follows:\n",
    "\n",
    "<img src=\"jupyter_imgs/gan.png\" alt=\"Algorithm\" style=\"width: 500px;\"/>\n",
    "<center> Figure 1. GAN Visual [8]</center>\n",
    "\n",
    "\n",
    "As shown in the image the entire architeture takes in a random noise vector of size 100 and produces a final output image, and decision from the Discriminator. The Discriminator will also take in the artworks from the original dataset in order to train on its art/not-art decision. The Generator, however sees only the decision of Discriminator and will backprop accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> 2. Related Work </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 The DC-GAN\n",
    "\n",
    "The state of the art GAN generator was created by Alec Radford & Luke Metz in 2017 [1]. It utilizes multiple layers of convolutions in two neural-net's to generate novel images by training on an image dataset. In their paper \"Unsupervised representation learning with deep convolutional Generative adversarial networks\", Radford and Metz explain and explore how DC-GANs are able to learn a hier-archy of representations about some image set which it can emulate to a degree much greater than its regular gan peers. \n",
    "\n",
    "### 2.2 The ArtGAN [4]\n",
    "\n",
    "In a paper by Tan *.et al* [4] researchers found that GANs which were conditioned on specific genres of art out performed regular DC-GANs at image generation. I found that their research was very interesting, but I wished they had a bit more of explaination of why that was the case, which I hope my project would shed some light on. They also used a DC-GAN but conditoned its input on an embedding a specific genre.\n",
    "\n",
    "### 2.3 Generative Adversarial Text to Image Synthesis [10]\n",
    "\n",
    "Reed *.et al* recently did a study on using text captions to both generate images following that caption and images of higher quality than a regular dc-gan. This is the most interesting research done, and applying this to artwork synthesis would prove to be a very interesting endeavor. I think that the next step in this project following this specific research would be an attempt to emulate this procedure (which I will do in 231n). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> 3. Dataset and Features </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I sample from the WikiArt dataset, which consists of 95000+ artworks, in order to train my gan. The data consists of images spanning all genres of art. \n",
    "\n",
    "![\"System\"](jupyter_imgs/wiki-art.png \"System Architecture\")\n",
    "\n",
    "<center> *Fig 2. Visual of the WikiArt dataset taken from [3] *</center>\n",
    "\n",
    "Following the work done by Tan *.et al* [4] on Categorical GANs, I seperated the artworks into the following genres:\n",
    "\n",
    "1. cityscape\n",
    "2. flower-painting\n",
    "3. mythological-painting\n",
    "4. still-life\n",
    "5. animal-painting\n",
    "6. figurative\n",
    "7. marina\n",
    "8. symbolic-painting\n",
    "9. abstract\n",
    "10. genre-painting\n",
    "11. landscape\n",
    "12. nude-painting-nu\n",
    "13. portrait\n",
    "14. religious-painting \n",
    "\n",
    "(by using [5]). \n",
    "\n",
    "The images themselves are much too large to pass as is to the GAN, therefore as another preprocessing step, I downsampled the images to be 64x64 to match the size of the artwork to be generated by the generator.\n",
    "\n",
    "To illustrate why this is a good move, consider the following t-distributed stochastic neighbor embedding (t-SNE) created on the WikiArt Dataset (Figure 2). As you can see in the diagram, which constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects have a high probability of being picked, there seem to be clusters of images around certain genres, such as the most upper cluster being that of sky paintings, and the most central that of portraiture.\n",
    "\n",
    "![\"System\"](jupyter_imgs/tsne.png \"t-SNE\")\n",
    "<center> * Fig 3. a t-SNE visualization of the WikiArt Dataset [9] *</center>\n",
    "\n",
    "With this clustering in mind we can see that there are disparate clusters of artistic style that the GAN will be able to detect, therefore if we narrow down on a specific style we might be able to better confuse the Discriminator and Generate more realistic works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> 4. Methods </center>\n",
    "\n",
    "In order to generate high quality color images, I used a Deep Convolutional Generative Adversarial Network (DCGAN) as proposed in [6]. Since Convolutional filters are used in a DCGAN meaning it is more able to learn the features associated with artwork. \n",
    "\n",
    "By seperating out the artworks into specific categories I hope to reduce the latent space of the data, such that the GAN is more able to generate within that specific space, creating more realistic works of a specific genre. \n",
    "\n",
    "#### 4.1 GAN Algorithm\n",
    "\n",
    "The following algorithm descibes how the mini-max game is trained and optimized.\n",
    "\n",
    "<img src=\"jupyter_imgs/algorithm2.png\" alt=\"Algorithm\" style=\"width: 500px;\"/>\n",
    "<center> Figure 4. Algorithm for GAN learning [7]</center>\n",
    "\n",
    "\n",
    "There are two loss functions that are being optimized on: The Generator is attmepting to minimize:\n",
    "\n",
    "$$\\mathop{\\mathbb{E}}_{z \\text{~} p_{z}(z)} [\\text{log}(1 - D(G(x)))] $$\n",
    "\n",
    "While the Discriminator maximizes:\n",
    "\n",
    "$$\\mathop{\\mathbb{E}}_{x, z \\text{~} p_{data}(x)} [\\text{log}D(x) + \\text{log}(1 - D(G(x)))] $$\n",
    "\n",
    "Making the entire loss function:\n",
    "\n",
    "$$\\min_G \\max_D V(D, G) = \\mathop{\\mathbb{E}}_{x \\text{~} p_{data}(x)} [\\text{log}D(x)] + \\mathop{\\mathbb{E}}_{z \\text{~} p_{z}(z)} [\\text{log}(1 - D(G(x)))]  $$\n",
    "\n",
    "With this the Generator attempts to create artworks that the discriminator thinks are art by trying to minimize the probability of a \"not art\" guess (minimizew the expectation of 1 - D(G(x))). The Discriminator is compaeting with this by trying to maximize the BCE loss of it making the right choice (ie knowing that the generator is producing fake \"not art\" and that the real image data is real art). By competing the Generator will attempt to create works that trick the Discriminator and therefore be realistic looking artworks.\n",
    "\n",
    "\n",
    "#### 4.2 DC-GAN\n",
    "\n",
    "In a DC-GAN the layers of the Generative adversarial network are convolutional layers. Thus they opperate using convolutional filters over every intermediate output. \n",
    "\n",
    "![\"System\"](jupyter_imgs/System.png \"System Architecture\")\n",
    "\n",
    "<center> $$\\text{Figure 5. Architecture of a GAN Generator and Discriminator}^{1}$$</center>\n",
    "\n",
    "Convolutional Neural Nets (CNNs) are able to discover alot of detail about some space, due to the large number of parameters and large number of filters. This is particularly helpful here since artwork varies very drastically, even within a genre of art, meaning that the more knowledge the GAN can extract from the artworks the better.\n",
    "\n",
    "#### 4.3 Training\n",
    "\n",
    "As mentioned above, in an attempt to create more realistic images aligning with some genre of artwork, I seperated the data into genre specific samples and trained the GAN on the large image dataset that each genre had. I used Stochastic Gradient descent with an Adam optimizer and a learning rate of 0.0002. \n",
    "\n",
    "#### 4.4 Scoring\n",
    "\n",
    "For art generation the best metric is that of the expert human eye, since we - as humans - developed the concept of aesthetic beauty and artwork quality. While the best, it is the least accessible metric, and requires alot of time and co-operation. Therefore some other metrics need to be used to approximate the experts eye. \n",
    "\n",
    "I will be using the following metric to measure the outcomes of my GAN as opposed to that of its predecessors:\n",
    "\n",
    "1. Inception Score: The inception score is the most commonly adopted metric for GAN performance, it uses conditional and marginal distributions over generated data or real data and is able to evaluate diversity and fidelity of samples. There is however evidence that shows that it might favor over fitting [2] the original data-set. Another probblem with the inception score is that it is measuring the validity of some artwork (genre) in a pretrained data-set. This means that between genres relative inception scores might not scale exactly. Eg, if I run the inception network on the landscape painting dataset it scores only a 3.1 compared to the 7.7 that the animal-painting data set images score. To overcome this I am going to look at the relative score of the generated work to its genre-d training set score\n",
    "\n",
    "2. K-Nearest Neighbors (KNN): By using KNN we can determing if the output images are overfitting the original dataset. By using this as a good measure of over fit alongside the Inception Score [2], I will, hopefully be able to, balance overfit and good performance. I used a KNN of 1 to see if the image was resembling a single image in the output. The score is then the smallest euclidean distance of the generated work to the dataset\n",
    "\n",
    "###### **1.Images drawn using software called \"NN-SVG\" found here: http://alexlenail.me/NN-SVG/AlexNet.html**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> 5. Code </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** !!!Requires python 2.7!!!** First, lets import the necessary requirements to run the GAN as a setup process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "# this is a self defined function\n",
    "from inception import inception_score\n",
    "from PIL import ImageFile, Image\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets define a couple of constants used within the GAN during training. Take note of data-root, this is where the images of a certain genre will need to be imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = './images/animal-painting'\n",
    "workers = 1 # the number of workers used in parallel to load the data\n",
    "batchSize = 64 # number of images to create in each batch\n",
    "imageSize = 64 # size of images output\n",
    "nz = 100 # size of input noise to the generator\n",
    "ngf = 64 # hidden size for the generator\n",
    "ndf = 64 # hidden size for the discriminator\n",
    "niter = 250 # number of iterations\n",
    "nc = 3 # number of channels in input images (R, G, B)\n",
    "lr = 0.0002\n",
    "beta1 = 0.5 # for adam optim\n",
    "outf = './out/animal_out'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next create a dataloader for the image folder found in dataroot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dset.ImageFolder(root = dataroot,\n",
    "                           transform = transforms.Compose([\n",
    "                               transforms.Resize(imageSize), # downsample the images to be the same size as generated\n",
    "                               transforms.CenterCrop(imageSize),\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ]))\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size= batchSize,\n",
    "                                         shuffle=True, num_workers=int( workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the generator and discriminator modules to be that as listed in [6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then have to initialize the Generator and Discriminator to be zeroed out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d (100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): ConvTranspose2d (512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): ConvTranspose2d (256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): ConvTranspose2d (128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): ConvTranspose2d (64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh()\n",
      "  )\n",
      ")\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d (3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(0.2, inplace)\n",
      "    (2): Conv2d (64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (4): LeakyReLU(0.2, inplace)\n",
      "    (5): Conv2d (128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (7): LeakyReLU(0.2, inplace)\n",
      "    (8): Conv2d (256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (10): LeakyReLU(0.2, inplace)\n",
      "    (11): Conv2d (512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)\n",
    "        \n",
    "netG = Generator(ngpu)\n",
    "netG.apply(weights_init)\n",
    "print(netG)\n",
    "\n",
    "netD = Discriminator(ngpu)\n",
    "netD.apply(weights_init)\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I train the GAN using Adam optimizer, doing numiter epochs and looping through all images in the real data. Calculate the loss function: \n",
    "\n",
    "$$\\min_G \\max_D V(D, G) = \\mathop{\\mathbb{E}}_{x \\text{~} p_{data}(x)} [\\text{log}D(x)] + \\mathop{\\mathbb{E}}_{z \\text{~} p_{z}(z)} [\\text{log}(1 - D(G(x)))]  $$\n",
    "\n",
    "and backprop on the loss for both the generator and the discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specific loss function for this minimax bce loss\n",
    "criterion = nn.BCELoss()  \n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# setup optimizer\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "for epoch in range(niter):\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        # train with real\n",
    "        netD.zero_grad()\n",
    "        real_cpu = data[0]\n",
    "        batch_size = real_cpu.size(0)\n",
    "        label = torch.full((batch_size,), real_label, device=device)\n",
    "\n",
    "        output = netD(real_cpu)\n",
    "        errD_real = criterion(output, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        # train with fake\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, label)\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        optimizerG.step()\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, opt.niter, i, len(dataloader),\n",
    "                 errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally run the loss function. I used inception loss as a metric of performance of the GANs generated artworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_inception_score(imgs, name):\n",
    "    data = torch.utils.data.TensorDataset(imgs)\n",
    "    mean, std = inception_score(data, cuda=opt.cuda)\n",
    "    print(name + 'inception mean: %.4f' % mean)\n",
    "    print(name + 'inception std: %.4f' % std)\n",
    "\n",
    "# generate some noise and evaluate the perfomance of the GAN\n",
    "noise = torch.randn(batchSize, nz, 1, 1)\n",
    "get_inception_score(netG(noise), 'Generated ')\n",
    "get_inception_score(real_imgs, 'Real Images ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> 6. Experiments </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Experiments: Baseline DC-GAN\n",
    "\n",
    "As an initial test I ran my DCGAN on a random selection of around 3000 images sampling from all of the different genres of art and recived the following results after 250 epochs\n",
    "\n",
    "Here is a set of samples from the random dataset of images:\n",
    "\n",
    "<img src=\"jupyter_imgs/random_real_samples.png\" alt=\"Random\" style=\"width: 400px;\"/>\n",
    "<center> Figure 6. Sampled images from random dataset</center>\n",
    "\n",
    "##### 6.1.1 Baseline DC-GAN Qualitative Results\n",
    "\n",
    "<img src=\"jupyter_imgs/random_out_epoch_249.png\" alt=\"Random\" style=\"width: 400px;\"/> \n",
    "<center> Figure 7. Baseline out trained on 3000 random images for 250 epochs</center>\n",
    "\n",
    "##### 6.1.2 Baseline DC-GAN Quantitative Results\n",
    "\n",
    "The Baseline had the following scores:\n",
    "\n",
    "`Inception: 3.1899`\n",
    "\n",
    "`KNN: 42.9972`\n",
    "\n",
    "This is compared to the following real sample inception score:\n",
    "\n",
    "`Inception: 5.7336`\n",
    "\n",
    "Making the relative inception error: `0.5563`\n",
    "\n",
    "\n",
    "### 6.2 Experiments:  Genre-Specific DC-GAN on small input\n",
    "\n",
    "Following this I ran my DCGAN on a selection of around 2800 images sampling from only from the specific genre of flower-paintings.\n",
    "\n",
    "Here is a set of samples from the random dataset of images:\n",
    "\n",
    "<img src=\"jupyter_imgs/flower_real_samples.png\" alt=\"Random\" style=\"width: 400px;\"/>\n",
    "<center> Figure 8. Sampled images from flower-painting dataset</center>\n",
    "\n",
    "##### 6.1.1 Genre-Specific DC-GAN on small input Qualitative Results\n",
    "\n",
    "<img src=\"jupyter_imgs/flower_epoch_236.png\" alt=\"Random\" style=\"width: 400px;\"/> \n",
    "<center> Figure 9. Baseline out trained on 2800 flower-painting images for 250 epochs</center>\n",
    "\n",
    "##### 6.1.2 Genre-Specific DC-GAN on small input Quantitative Results\n",
    "\n",
    "The Genre-Specific DC-GAN had the following scores:\n",
    "\n",
    "`Inception: 2.8298`\n",
    "\n",
    "`KNN: 38.2218`\n",
    "\n",
    "This is compared to the following real sample inception score:\n",
    "\n",
    "`Inception: 4.6301`\n",
    "\n",
    "Making the relative error: `0.6113`\n",
    "\n",
    "### 6.2 Experiments:  Genre-Specific DC-GAN on large input\n",
    "\n",
    "Following this I ran my DCGAN on a selection of around 15000 images sampling from only from the specific genre of landscape paintings.\n",
    "\n",
    "Here is a set of samples from the random dataset of images:\n",
    "\n",
    "<img src=\"jupyter_imgs/landscape_real_samples.png\" alt=\"Random\" style=\"width: 400px;\"/>\n",
    "<center> Figure 10. Sampled images from landscape dataset</center>\n",
    "\n",
    "##### 6.1.1 Genre-Specific DC-GAN on large input Qualitative Results\n",
    "\n",
    "<img src=\"jupyter_imgs/landscape_epoch_249.png\" alt=\"Random\" style=\"width: 400px;\"/> \n",
    "<center> Figure 11. Baseline out trained on 15000 landscape painting images for 250 epochs</center>\n",
    "\n",
    "##### 6.1.2 Genre-Specific DC-GAN on large input Quanitative Results\n",
    "\n",
    "The larger Genre-Specific DC-GAN had the following scores:\n",
    "\n",
    "`Inception: 2.4678`\n",
    "\n",
    "`KNN: 34.1319`\n",
    "\n",
    "This is compared to the following real sample inception score:\n",
    "\n",
    "`Inception: 3.1783`\n",
    "\n",
    "Making the relative error: `0.7764`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> 7. Results/Conclusion </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this research experiment I conculed that, by chosing genre specific inputs, we can create more realistic sets of artworks that are both of a specific genre and have a better relative inception score to the inception scores of the images in its dataset and are thus more realistic works of art themselves. I have also seen that the larger the number of images input into the Generator the better both the generated image and the less likely the image to be too baised to the input.  \n",
    "\n",
    "Initially I had hoped that the genre specific GANs would have higher inception scores overall but that didnt seem to entirely be the case since I didn't necessarily keep the database exactly the same.\n",
    "\n",
    "Qualitatively the images are much more artistic in the larger genre specific set which is a good sign.\n",
    "\n",
    "The KNN scores seem to get worse as we train on much larger genre specific outputs which means that bias seems to be going up. This is an interesting topic to study in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> 8. Discussion/Future Work </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Future Directions\n",
    "\n",
    "1. With a better data-base that is labeled and has a better set of categories to seperate the entire artwork database I believe that there could be more research done into querying specific types of artwork and generating better results from there. Such datasets could be something like BAM (The Behance Image Dataset) which are crowd-source labeled images, but require a request to access.\n",
    "2. GANs are extremely time inefficeint, the final output I created took over 24+ hours even when using 2 Nvidia GPUs and an 8 core machine, obviously more compute power would allow for a faster turn around of results and therefore a but more tweaking and twisting to get the best data outcomes.\n",
    "3. By having more compute I would also be able to use produce larger images, which should definitely be of better quality than that of the smaller pixel images.\n",
    "4. Run the GAN on different size samples from the same dataset to determine if the inception score relatively does go up with more images present\n",
    "5. Study the effects of larger input and higher bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://arxiv.org/pdf/1511.06434.pdf\n",
    "\n",
    "[2] https://arxiv.org/pdf/1711.10337.pdf\n",
    "\n",
    "[3] http://www.jdl.ac.cn/doc/2011/201811120455532723_p1174-ma.pdf\n",
    "\n",
    "[4] https://arxiv.org/pdf/1702.03410.pdf\n",
    "\n",
    "[5] https://github.com/lucasdavid/wikiart\n",
    "\n",
    "[6] https://arxiv.org/pdf/1511.06434.pdf\n",
    "\n",
    "[7] https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f\n",
    "\n",
    "[8] https://medium.freecodecamp.org/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394\n",
    "\n",
    "[9] http://www.jdl.ac.cn/doc/2011/201811120455532723_p1174-ma.pdf\n",
    "\n",
    "[10] https://arxiv.org/pdf/1711.10337.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
